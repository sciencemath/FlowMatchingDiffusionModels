{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e6ea05-134c-4c0c-a7ef-7188abe6d8c7",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b3b3e-e5c0-423a-93bc-7dd79376fa8e",
   "metadata": {},
   "source": [
    "understanding this is key, so a little background should make things a little clearer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f46d5-4d0c-4371-bdc0-4b6f676eca49",
   "metadata": {},
   "source": [
    "# **How Diffusion Models Create New Data from Noise**\n",
    "\n",
    "The diffusion model works by starting with **real data**, adding noise to it, and then learning how to reverse that noise to get back to the original data. However, creating something **new**‚Äîsuch as generating images from text‚Äîhappens during the **generation phase**, after the model has been trained.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Training Phase (Learning How to Denoise)**\n",
    "\n",
    "- The **training process** for a diffusion model involves teaching it how to **reverse the noise** it has added to real data.\n",
    "  \n",
    "- During training, the model is fed real data (e.g., images), and at each timestep, noise is gradually added to it (forward process). The model learns how to reverse this process (the reverse diffusion) so it can recover the clean data (original image) from noisy versions.\n",
    "\n",
    "- The model **does not create new data** during training. It simply learns how to **denoise** noisy data by learning the relationship between noisy data and clean data.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Generation Phase (Creating New Data)**\n",
    "\n",
    "Once trained, the model can **generate new data** by starting from **pure noise** and applying the learned denoising process step by step to turn that noise into meaningful output (e.g., an image, text, etc.).\n",
    "\n",
    "### **From Text to Image (e.g., Text-to-Image Diffusion Models)**\n",
    "\n",
    "1. **Start with Random Noise**: The model begins with a **random noise image**. This image is essentially a collection of random pixels with no meaningful structure or pattern.\n",
    "\n",
    "2. **Input Text**: In a **text-to-image model**, the model receives a text description (e.g., \"A sunset over the mountains\"). This text description is used to guide the generation process.\n",
    "\n",
    "3. **Conditioning the Model on Text**: The model is **conditioned on the text** using a mechanism like **CLIP** (Contrastive Language-Image Pre-training), or a similar architecture, to understand the semantic meaning of the text. This helps the model know what type of image it should generate from the noise, based on the input description.\n",
    "\n",
    "4. **Gradual Denoising**: The model now **denoises the random image step-by-step**, guided by the textual description. At each step, the model makes the image less noisy and more structured, eventually generating an image that matches the description in the text.\n",
    "\n",
    "   - At each timestep of the reverse process, the model **refines** the image by removing noise and introducing more structure according to what the text describes (e.g., \"sunset,\" \"mountains,\" \"sky\").\n",
    "\n",
    "5. **End Result**: After several steps of denoising, the random noise transforms into a **coherent image** that matches the text prompt. This is how the model creates a new image that it has never seen before!\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Does This Work?**\n",
    "The power of diffusion models comes from their ability to **learn the underlying structure of data** (e.g., images, audio, etc.) during training. During the reverse diffusion process, the model can apply that learned structure to **generate completely new data** from random noise, guided by the text or other inputs.\n",
    "\n",
    "In text-to-image generation:\n",
    "- The **conditioning mechanism** (like CLIP) ensures that the generated image aligns with the text description.\n",
    "- The **denoising process** ensures that the random noise is transformed into a **coherent and meaningful output**.\n",
    "\n",
    "This is how diffusion models are able to generate **new images** that have never been seen before based on an input prompt (text, in this case).\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of the Process:**\n",
    "1. **Training**: The model learns to reverse the noise from real data.\n",
    "2. **Generation**: The model starts with noise and uses learned denoising steps, guided by input (like text), to generate **entirely new** data.\n",
    "\n",
    "Thus, the model **creates something new** because during training, it has learned how to **extract meaningful patterns** from noisy data and use that knowledge to generate new examples from noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea3f47-6656-42b5-897f-5af13f0ffdb7",
   "metadata": {},
   "source": [
    "# **Marginal Probability Path and Its Relation to Conditional Probability Path**\n",
    "\n",
    "## **1. From Conditional to Marginal Probability**\n",
    "The **conditional probability path** expresses how one event depends on another:\n",
    "\n",
    "$$\n",
    "P(A_n | A_{n-1}, A_{n-2}, \\dots, A_1) = \\frac{P(A_n, A_{n-1}, \\dots, A_1)}{P(A_{n-1}, \\dots, A_1)}\n",
    "$$\n",
    "\n",
    "From this, we can express the **joint probability** using the chain rule:\n",
    "\n",
    "$$\n",
    "P(A_1, A_2, \\dots, A_n) = P(A_1) P(A_2 | A_1) P(A_3 | A_1, A_2) \\dots P(A_n | A_1, A_2, \\dots, A_{n-1}).\n",
    "$$\n",
    "\n",
    "To obtain the **marginal probability** of a specific variable (say \\( A_n \\)), we sum (or integrate) over all other variables:\n",
    "\n",
    "$$\n",
    "P(A_n) = \\sum_{A_1, A_2, \\dots, A_{n-1}} P(A_n, A_{n-1}, \\dots, A_1).\n",
    "$$\n",
    "\n",
    "or in continuous form:\n",
    "\n",
    "$$\n",
    "P(A_n) = \\int P(A_n, A_{n-1}, \\dots, A_1) \\, dA_1 \\, dA_2 \\, \\dots \\, dA_{n-1}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Does This Matter?**\n",
    "- **Marginalization removes dependencies** and gives us the probability of a variable without needing to know the others.\n",
    "- **In Bayesian inference**, marginal probabilities (evidence) allow us to normalize posterior distributions:\n",
    "  \n",
    "  $$\n",
    "  P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n",
    "  $$\n",
    "\n",
    "  where \\( P(B) \\) is found by marginalizing over all possible values of \\( A \\).\n",
    "\n",
    "- **In Markov Models**, marginal probability paths give the probability of being in a certain state at a given time.\n",
    "- **In diffusion models**, marginalizing over noise variables provides a transition from Gaussian noise back to the data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Intuition: How Conditional Leads to Marginal**\n",
    "1. **Start with conditional probabilities (dependencies between variables).**\n",
    "2. **Expand them using the chain rule.**\n",
    "3. **Integrate or sum out unwanted variables to get marginal probabilities.**\n",
    "\n",
    "This progression is fundamental in probabilistic AI models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8aae0-6bbe-40ce-a029-5bccb1f3b785",
   "metadata": {},
   "source": [
    "# **Gaussian Probability Path and Its Importance**\n",
    "\n",
    "## **1. What is the Gaussian Probability Path?**  \n",
    "\n",
    "A **Gaussian probability path** describes a **sequence of probability distributions** where each step follows a Gaussian distribution. This is common in **stochastic processes, diffusion models, and AI applications**.\n",
    "\n",
    "If we model a random variable $X_t$ evolving over time, and at each step its probability distribution remains Gaussian, we define its probability path as:\n",
    "\n",
    "$$\n",
    "P(X_t) = \\mathcal{N}(\\mu_t, \\sigma_t^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the mean (center of the distribution).\n",
    "- $\\sigma_t^2$ is the variance (spread of the distribution).\n",
    "- $t$ represents a step in time.\n",
    "\n",
    "This means that at **each point in time, the distribution of $X_t$ is Gaussian**, but its parameters $(\\mu_t, \\sigma_t)$ may change over time.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How is it Formulated?**\n",
    "A common way to describe the evolution of a **Gaussian probability path** is through **stochastic differential equations (SDEs)**:\n",
    "\n",
    "$$\n",
    "dX_t = \\mu_t dt + \\sigma_t dW_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $dX_t$ is the infinitesimal change in $X_t$.\n",
    "- $\\mu_t$ is the drift term (mean shift).\n",
    "- $\\sigma_t$ is the diffusion term (variance change).\n",
    "- $dW_t$ is a **Wiener process** (random noise).\n",
    "\n",
    "This equation describes **Brownian motion with drift**, meaning $X_t$ follows a Gaussian distribution that shifts and spreads over time.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Why is the Gaussian Probability Path Important?**\n",
    "### **(a) Diffusion Models in AI**\n",
    "- **Diffusion probabilistic models** (e.g., Denoising Diffusion Probabilistic Models, DDPMs) use Gaussian probability paths to gradually **add noise to data** and then **reverse the process** to generate realistic samples.\n",
    "- Example: Image generation in **Stable Diffusion**.\n",
    "\n",
    "### **(b) Bayesian Filtering (Kalman Filter)**\n",
    "- **Kalman filters** use Gaussian probability paths to track systems with uncertainty.\n",
    "- Example: **Predicting the location of an object over time**.\n",
    "\n",
    "### **(c) Stochastic Processes in Finance**\n",
    "- **Stock price modeling** (e.g., Black-Scholes equation) assumes **Gaussian random walks** for price movements.\n",
    "\n",
    "### **(d) Interpolation and Noise Modeling**\n",
    "- Gaussian probability paths **smoothly interpolate** between data points and approximate noise distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Summary**\n",
    "| Concept | Explanation |\n",
    "|---------|------------|\n",
    "| **Definition** | A path where a system's probability distribution remains Gaussian at each step. |\n",
    "| **Equation** | $P(X_t) = \\mathcal{N}(\\mu_t, \\sigma_t^2)$, evolving via $dX_t = \\mu_t dt + \\sigma_t dW_t$. |\n",
    "| **Importance** | Used in AI (diffusion models), Bayesian filtering (Kalman), finance (Black-Scholes), and stochastic processes. |\n",
    "| **Key Idea** | Describes how a system evolves probabilistically while maintaining a Gaussian shape. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eacda77-aba0-419c-b314-de8865a7ec67",
   "metadata": {},
   "source": [
    "# **How $ \\alpha_t $ and $ \\beta_t $ Work in Denoising and Noising**\n",
    "\n",
    "In **diffusion models**, $ \\alpha_t $ and $ \\beta_t $ are key components of the **forward** and **reverse diffusion processes**. These functions help control the process of **adding noise** to the data and then **removing it** during training, enabling the model to learn how to recover data from noisy versions.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Forward Process (Adding Noise)**\n",
    "- During the **forward diffusion** process, noise is gradually added to the data.  \n",
    "- $ \\alpha_t $ defines how much of the original signal remains at each timestep, and $ \\beta_t $ controls how much noise is added at that timestep.  \n",
    "- At the start ($ t = 0 $), there is no noise, and at the end ($ t = 1 $), the data is almost pure noise. This process allows the model to learn how the data transitions from a clean signal to noise over time.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Reverse Process (Removing Noise)**\n",
    "- In the **reverse diffusion** process, the model learns how to denoise, or reverse the added noise, at each timestep.  \n",
    "- The goal is for the model to effectively \"learn\" how to reverse the noise by using the learned noise schedule $ \\alpha_t $ and $ \\beta_t $, and thus recover the original data from the noisy versions.  \n",
    "- This reverse process requires the model to predict the clean data based on the noisy data at each timestep, and it's **learned during training** by minimizing the difference between the predicted clean data and the original data.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Training with $ \\alpha_t $ and $ \\beta_t $**\n",
    "- Through **training** with noisy data and the noise scheduling functions, the model learns to predict the clean data at each timestep of the reverse process.\n",
    "- Once trained, the model can **generate new data** (e.g., images, text, etc.) by starting from noise and using the learned reverse process to denoise step by step. This allows the model to **generate data from scratch**, guided by the noise schedule.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways:**\n",
    "- $ \\alpha_t $ and $ \\beta_t $ allow the model to **learn the process of adding and removing noise** effectively.\n",
    "- Once the model has learned the forward and reverse processes, it can **generate new data** (e.g., creating new images or data points) by using the reverse process starting from noise.\n",
    "- This makes the model capable of **training on noisy data** and learning how to recover and generate clean data, which is especially powerful for tasks like **image generation**, **data denoising**, and other generative tasks.\n",
    "\n",
    "So, in short, these functions help the model learn how to **denoise and add noise** in a controlled manner, and in the future, the model can use this learned knowledge to train on new noisy data and even generate entirely new data from noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1633871f-3db2-4537-bb44-0cca20e00e62",
   "metadata": {},
   "source": [
    "# **Why Are $ \\alpha_t $ and $ \\beta_t $ Used as Noise Schedulers?**\n",
    "\n",
    "In **diffusion models** and **probabilistic paths**, $ \\alpha_t $ and $ \\beta_t $ are used as **noise scheduling functions** that control how noise is gradually added and removed during training and inference. These functions must be **continuously differentiable, monotonic**, and satisfy boundary conditions:\n",
    "\n",
    "$$\n",
    "\\alpha_0 = \\beta_1 = 0, \\quad \\alpha_1 = \\beta_0 = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What Are $ \\alpha_t $ and $ \\beta_t $?**  \n",
    "- **$ \\alpha_t $ (Signal Retention Factor)**  \n",
    "  - Determines **how much of the original signal remains** at time $ t $.  \n",
    "  - Starts from 1 (clean data) and gradually decreases to 0 (pure noise).  \n",
    "\n",
    "- **$ \\beta_t $ (Noise Schedule Factor)**  \n",
    "  - Controls **how much noise is added** at time $ t $.  \n",
    "  - Starts from 0 (no noise) and increases to 1 (full noise).  \n",
    "\n",
    "Mathematically, these functions define a smooth transition between the clean data and a Gaussian noise distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Must $ \\alpha_t $ and $ \\beta_t $ Be Monotonic and Differentiable?**\n",
    "### **(a) Ensures a Smooth Noise Transition**  \n",
    "- **Monotonicity** ($ \\alpha_t $ decreasing, $ \\beta_t $ increasing) guarantees that we smoothly **add noise** during the forward diffusion process and **remove noise** in the reverse process.  \n",
    "- Without monotonicity, we could accidentally **reintroduce the signal too early** or **add noise inconsistently**.\n",
    "\n",
    "### **(b) Ensures a Well-Defined Probability Path**  \n",
    "- A **continuously differentiable** function allows the diffusion process to be solved using **stochastic differential equations (SDEs)** and **ODEs**.\n",
    "- This ensures a **smooth interpolation** between data points when denoising.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How Are $ \\alpha_t $ and $ \\beta_t $ Defined?**\n",
    "A common choice is using a **logarithmic or polynomial noise schedule**, such as:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\cos^2 \\left( \\frac{\\pi}{2} t \\right), \\quad \\beta_t = 1 - \\alpha_t\n",
    "$$\n",
    "\n",
    "or linearly:\n",
    "\n",
    "$$\n",
    "\\beta_t = \\beta_{\\text{min}} + (\\beta_{\\text{max}} - \\beta_{\\text{min}}) t\n",
    "$$\n",
    "\n",
    "where $ \\beta_{\\text{min}} $ and $ \\beta_{\\text{max}} $ define the noise intensity range.\n",
    "\n",
    "These functions **ensure a gradual and smooth transition** from clean data to pure noise.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Summary Table**\n",
    "| Property | $ \\alpha_t $ (Signal) | $ \\beta_t $ (Noise) |\n",
    "|----------|------------------|------------------|\n",
    "| **Definition** | Controls **how much signal remains** at time $ t $ | Controls **how much noise is added** at time $ t $ |\n",
    "| **Behavior** | Starts at 1, decreases to 0 | Starts at 0, increases to 1 |\n",
    "| **Monotonicity** | Always **decreasing** | Always **increasing** |\n",
    "| **Differentiability** | **Continuously differentiable** to ensure smooth evolution | Same as $ \\alpha_t $ |\n",
    "| **Why Important?** | Prevents signal from disappearing too quickly | Prevents noise from being added too aggressively |\n",
    "\n",
    "---\n",
    "\n",
    "This is why **$ \\alpha_t $ and $ \\beta_t $ are key noise schedulers** in diffusion models. They ensure a **controlled, smooth** transition between **clean data** and **Gaussian noise**, making the reverse process **learnable**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2839a60c-fac8-415d-bdc9-7e37d5708cec",
   "metadata": {},
   "source": [
    "# Difference Between Flow Matching Loss and Conditional Matching Loss\n",
    "Flow Matching Loss\n",
    "\n",
    "This is used in continuous normalizing flows (CNFs) or diffusion models, where we aim to match the probability flow of data across timesteps.\n",
    "The loss ensures that the velocity field of the model (how data transforms over time) is close to the true velocity of data under a given distribution shift.\n",
    "\n",
    "Conditional Matching Loss\n",
    "\n",
    "This is an extension where the loss depends on additional conditions (e.g., class labels, external factors).\n",
    "Instead of just matching flows blindly, the model is trained to match flows given a condition (e.g., denoising an image given a blurry input).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fdb75-7f79-412e-8ceb-c75efa63d7d3",
   "metadata": {},
   "source": [
    "# Flow Matching Loss vs. Conditional Matching Loss\n",
    "\n",
    "## 1Ô∏è‚É£ Flow Matching Loss Example: Moving a Distribution\n",
    "Imagine you have **data points that follow a Gaussian distribution** (mean = 0, variance = 1), and you want to learn how these points move over time.\n",
    "\n",
    "### Step 1: Sample Data Points\n",
    "Let's say we have three sample points at an initial time step:\n",
    "\n",
    "$$\n",
    "x_1 = -1.5, \\quad x_2 = 0.3, \\quad x_3 = 2.0\n",
    "$$\n",
    "\n",
    "### Step 2: Compute True Velocity\n",
    "In flow matching, we model how each point **should move** over time based on the probability density function.  \n",
    "For a Gaussian, the true velocity is simply the negative of the point value:\n",
    "\n",
    "$$\n",
    "v_{\\text{true}}(x) = -x\n",
    "$$\n",
    "\n",
    "So, for our points:\n",
    "\n",
    "$$\n",
    "v_{\\text{true}}(-1.5) = 1.5, \\quad v_{\\text{true}}(0.3) = -0.3, \\quad v_{\\text{true}}(2.0) = -2.0\n",
    "$$\n",
    "\n",
    "### Step 3: Model‚Äôs Predicted Velocity\n",
    "Let's say our model predicts these velocities instead:\n",
    "\n",
    "$$\n",
    "v_{\\text{pred}}(-1.5) = 1.2, \\quad v_{\\text{pred}}(0.3) = -0.5, \\quad v_{\\text{pred}}(2.0) = -1.8\n",
    "$$\n",
    "\n",
    "### Step 4: Compute Flow Matching Loss\n",
    "The loss is the squared difference between true and predicted velocities:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{3} \\left( (1.5 - 1.2)^2 + (-0.3 + 0.5)^2 + (-2.0 + 1.8)^2 \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{3} \\left( 0.09 + 0.04 + 0.04 \\right) = \\frac{0.17}{3} = 0.0567\n",
    "$$\n",
    "\n",
    "üëâ This loss tells the model how far its velocity predictions are from the correct flow.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Conditional Matching Loss Example: Denoising an Image\n",
    "Imagine you are trying to remove noise from grayscale image pixels.\n",
    "\n",
    "### Step 1: Sample Noisy Data\n",
    "You have a noisy pixel value (conditioned input):\n",
    "\n",
    "$$\n",
    "x_{\\text{noisy}} = 120\n",
    "$$\n",
    "\n",
    "And you know the true clean pixel should be:\n",
    "\n",
    "$$\n",
    "x_{\\text{clean}} = 100\n",
    "$$\n",
    "\n",
    "### Step 2: Model‚Äôs Predicted Clean Pixel\n",
    "Your denoising model predicts:\n",
    "\n",
    "$$\n",
    "x_{\\text{pred}} = 110\n",
    "$$\n",
    "\n",
    "### Step 3: Compute Conditional Matching Loss\n",
    "The loss is simply the squared difference:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = (100 - 110)^2 = 100\n",
    "$$\n",
    "\n",
    "üëâ The model learns to minimize this error, meaning it should predict cleaner values given noisy inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Difference\n",
    "\n",
    "| Loss Type                | What‚Äôs Matched?             | Example                              |\n",
    "|--------------------------|----------------------------|--------------------------------------|\n",
    "| **Flow Matching Loss**   | Velocities of data movement | Transforming noise into a Gaussian  |\n",
    "| **Conditional Matching Loss** | Data given a condition  | Denoising an image given a noisy input |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e367d8-1b02-482e-bec0-b65d130b8566",
   "metadata": {},
   "source": [
    "### **What is Score Matching and Why is it Important?**\n",
    "\n",
    "**Score Matching** is a method used to estimate the gradient of the log-likelihood of a probability distribution, also known as the \"score function.\" The score function provides the direction in which the likelihood of data is most sensitive to changes in the data. Estimating the score is crucial in machine learning and statistics, as it helps in understanding the data distribution and is used in generative models, such as in training energy-based models (EBMs).\n",
    "\n",
    "In Score Matching, the goal is to minimize the difference between the **true score function** and the **model's predicted score function**. This is done by training a model to match the gradient of the log-likelihood of the true distribution.\n",
    "\n",
    "The **Score Matching** objective is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SM} = \\mathbb{E}_{p(x)} \\left[ \\| \\nabla_x \\log p(x) - s_{\\theta}(x) \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p(x)$ is the true data distribution.\n",
    "- $s_{\\theta}(x)$ is the learned score function.\n",
    "- $\\nabla_x \\log p(x)$ is the true gradient of the log-likelihood.\n",
    "\n",
    "By minimizing this loss, the model learns to approximate the true score function, which is important for tasks like generative modeling and density estimation.\n",
    "\n",
    "### **Why is Score Matching Important?**\n",
    "\n",
    "Score matching is important because:\n",
    "1. **Gradient Estimation**: It provides a way to estimate the gradient of the log-likelihood without needing to explicitly compute the likelihood, which can be difficult in high-dimensional spaces.\n",
    "2. **Generative Modeling**: It is crucial in training energy-based models, where the goal is to generate data samples from a probability distribution without needing to normalize the distribution.\n",
    "3. **Improves Data Understanding**: By learning the score function, the model gains insight into the structure of the data distribution.\n",
    "\n",
    "### **Example of Score Matching with Numbers**\n",
    "\n",
    "Let‚Äôs consider a simple univariate Gaussian distribution with a known true score function. Suppose the data is drawn from a normal distribution $N(0, 1)$ with a mean of 0 and variance of 1. \n",
    "\n",
    "#### **True Distribution**\n",
    "\n",
    "For a univariate Gaussian distribution, the log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\log p(x) = -\\frac{1}{2} \\log(2 \\pi) - \\frac{1}{2} x^2\n",
    "$$\n",
    "\n",
    "The true score function, which is the gradient of the log-likelihood, is:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log p(x) = -x\n",
    "$$\n",
    "\n",
    "This is the true score function for $p(x) \\sim N(0, 1)$.\n",
    "\n",
    "#### **Model‚Äôs Score Function (Simplified Example)**\n",
    "\n",
    "Now, let‚Äôs assume our model predicts the score function as:\n",
    "\n",
    "$$\n",
    "s_{\\theta}(x) = -\\theta x\n",
    "$$\n",
    "\n",
    "where $\\theta$ is a parameter that the model learns.\n",
    "\n",
    "#### **Objective**\n",
    "\n",
    "The objective of score matching is to minimize the following loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SM} = \\mathbb{E}_{p(x)} \\left[ \\| \\nabla_x \\log p(x) - s_{\\theta}(x) \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "In our case, the true score is $\\nabla_x \\log p(x) = -x$, and the predicted score is $s_{\\theta}(x) = -\\theta x$. So, the loss function becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SM} = \\mathbb{E}_{p(x)} \\left[ \\| x(1 - \\theta) \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SM} = (1 - \\theta)^2\n",
    "$$\n",
    "\n",
    "#### **Minimizing the Loss**\n",
    "\n",
    "To minimize this loss, we set the derivative with respect to $\\theta$ equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} \\mathcal{L}_{SM} = 2(1 - \\theta)(-1) = 0\n",
    "$$\n",
    "\n",
    "This gives us $\\theta = 1$.\n",
    "\n",
    "Therefore, the optimal score function is obtained when $\\theta = 1$, which matches the true score function $\\nabla_x \\log p(x) = -x$.\n",
    "\n",
    "### **Summary**\n",
    "- **Score Matching** is a method for estimating the gradient of the log-likelihood (the score function) by minimizing the difference between the model‚Äôs predicted score and the true score.\n",
    "- It is important for generative modeling, density estimation, and when the likelihood is hard to compute directly.\n",
    "- The **loss function** used in score matching minimizes the difference between the true score function and the predicted score function, helping the model learn the underlying data distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7023f79-7f26-49dd-ba48-ddbad0bd3703",
   "metadata": {},
   "source": [
    "### **Difference Between Score Matching and Denoising Score Matching for Gaussian Probability Paths**\n",
    "\n",
    "Score Matching (SM) is a technique used to learn the score function (the gradient of the log-likelihood) of an unknown probability distribution. \n",
    "\n",
    "The **Score Matching** loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SM} = \\mathbb{E}_{p(x)}\\left[ \\|\\nabla_x \\log p(x) - s_{\\theta}(x)\\|^2 \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( p(x) \\) is the true distribution of data.\n",
    "- \\( s_{\\theta}(x) \\) is the learned score function, typically modeled using a neural network.\n",
    "- \\( \\nabla_x \\log p(x) \\) is the true gradient of the log-likelihood.\n",
    "\n",
    "**Denoising Score Matching (DSM)** is a variant of score matching where we deal with noisy data and the goal is to predict the clean data given noisy observations. This approach is more practical in real-world scenarios where the true data distribution is not easily accessible.\n",
    "\n",
    "The main difference lies in the **forward process**:\n",
    "\n",
    "- **Score Matching** involves directly comparing the score function of the data distribution.\n",
    "- **Denoising Score Matching** adds noise to the data and learns to denoise it by approximating the score of a noisy data distribution.\n",
    "\n",
    "In DSM, the model learns the score for the noisy data distribution \\( p_{\\text{noisy}}(x_t) \\), which is generated by adding Gaussian noise to clean data. For each time \\( t \\), a noisy version \\( x_t \\) of the clean data \\( x_0 \\) is created.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Denoising Score Matching (DSM) Leads to Diffusion Models**\n",
    "\n",
    "Denoising Score Matching (DSM) is a crucial step in the development of **score-based generative models**, including **diffusion models** (used in DALL¬∑E, Stable Diffusion, etc.). Let's break it down step by step.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1Ô∏è‚É£ Step 1: Why Not Model \\( p(x) \\) Directly?**\n",
    "The ultimate goal of generative modeling is to **sample from a complex distribution** \\( p(x) \\).  \n",
    "However, directly modeling \\( p(x) \\) is hard because:\n",
    "- The probability density function (PDF) may be unknown or difficult to normalize.\n",
    "- Computing the exact likelihood requires intractable integrals.\n",
    "\n",
    "Instead of learning \\( p(x) \\) explicitly, **we learn how \\( p(x) \\) changes**, which is given by the **score function**:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log p(x)\n",
    "$$\n",
    "\n",
    "This tells us the direction in which the density is increasing‚Äîjust like a gradient in optimization!\n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Step 2: Approximate Score Matching with DSM**\n",
    "Instead of estimating \\( \\nabla_x \\log p(x) \\) directly, **DSM estimates the score of a noisy version of \\( x \\)** by adding Gaussian noise:\n",
    "\n",
    "$$\n",
    "x_t = x + \\sigma_t \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x_t \\) is the noisy version of \\( x \\).\n",
    "- \\( \\sigma_t \\) is the noise level at time \\( t \\).\n",
    "- \\( \\epsilon \\) is the Gaussian noise.\n",
    "\n",
    "The model learns to predict the clean data \\( x \\) given \\( x_t \\), which indirectly learns:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log p_{\\text{noisy}}(x_t)\n",
    "$$\n",
    "\n",
    "which **approximates the true score function** \\( \\nabla_x \\log p(x) \\).\n",
    "\n",
    "üîπ **Why is this helpful?**  \n",
    "- Learning to remove noise **trains the model to follow the gradient of \\( p(x) \\) even when noise is present**.\n",
    "- This lets us **reverse the process of noise addition**, which is exactly what diffusion models do!\n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Step 3: Connecting DSM to Diffusion Models**\n",
    "Now, instead of just adding noise **once**, diffusion models **slowly add noise over many time steps** \\( t \\).  \n",
    "This creates a **probabilistic path** from real data to pure Gaussian noise:\n",
    "\n",
    "$$\n",
    "x_0 \\to x_1 \\to x_2 \\to \\dots \\to x_T \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x_0 \\) is real data.\n",
    "- \\( x_T \\) is pure Gaussian noise.\n",
    "- The process follows a **Markov Chain** where each step adds a tiny bit of noise.\n",
    "\n",
    "Since we learn the **score function at each noise level**, we can run this process **backward** to **generate new samples**!\n",
    "\n",
    "üîπ **Reverse Process (Sampling)**\n",
    "Once the model is trained to predict scores at different noise levels, we can start from **pure Gaussian noise** and use the estimated scores to gradually \"denoise\" it, step by step:\n",
    "\n",
    "$$\n",
    "x_T \\to x_{T-1} \\to \\dots \\to x_1 \\to x_0\n",
    "$$\n",
    "\n",
    "Eventually, this generates new samples from \\( p(x) \\), just like real data!\n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Step 4: Key Takeaways**\n",
    "‚úÖ **Score matching learns gradients of probability distributions instead of densities.**  \n",
    "‚úÖ **DSM makes score matching practical by learning to remove Gaussian noise.**  \n",
    "‚úÖ **Diffusion models extend DSM by applying noise in a step-by-step process.**  \n",
    "‚úÖ **Sampling is done by reversing the learned noise process, generating high-quality data.**  \n",
    "\n",
    "This is the foundation behind state-of-the-art **image, audio, and text generation models**! üöÄ  \n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Would You Like a Concrete Example with Numbers?**\n",
    "I can walk through an example where we **add noise, train a model to predict scores, and reverse the process** step by step! Let me know. üòÉ\n",
    "\n",
    "---\n",
    "\n",
    "### **Concrete Example: Denoising Score Matching (DSM) and Diffusion Models**\n",
    "\n",
    "Let's walk through a simplified example using numbers to better understand **Denoising Score Matching** and how it leads to **Diffusion Models**. We'll break it down into smaller steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Setup the Clean Data Distribution (Start with 1D Gaussian)**\n",
    "We begin with a simple 1D Gaussian distribution:\n",
    "\n",
    "$$\n",
    "x \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "This means the clean data $x$ is drawn from a normal distribution with mean 0 and variance 1. So, for example:\n",
    "\n",
    "$$\n",
    "x_0 = 1.5\n",
    "$$\n",
    "\n",
    "Now, let's add noise to $x_0$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Add Noise to $x_0$**\n",
    "For DSM, we add Gaussian noise at each step. Let‚Äôs assume we are at **time $t = 1$**, and we add noise with variance $\\sigma_1^2 = 0.2^2$.\n",
    "\n",
    "The noisy version of $x_0$ is:\n",
    "\n",
    "$$\n",
    "x_1 = x_0 + \\epsilon_1, \\quad \\epsilon_1 \\sim \\mathcal{N}(0, 0.2^2)\n",
    "$$\n",
    "\n",
    "For example, we could get:\n",
    "\n",
    "$$\n",
    "x_1 = 1.5 + 0.3 = 1.8\n",
    "$$\n",
    "\n",
    "Now, $x_1$ is noisy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Train a Model to Denoise**\n",
    "Now, let‚Äôs assume we have a model $s_{\\theta}(x_1)$ that learns to predict the clean $x_0$ given $x_1$. The idea is that the model learns the **score function** (gradient of log-likelihood) by minimizing the difference between the predicted score and the true score.\n",
    "\n",
    "The true score function at $x_1$ is the gradient of the log-density of the noisy distribution $p_{\\text{noisy}}(x_1)$, which is:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log p_{\\text{noisy}}(x_1) = \\frac{x_0 - x_1}{\\sigma_1^2}\n",
    "$$\n",
    "\n",
    "Using our numbers:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log p_{\\text{noisy}}(x_1) = \\frac{1.5 - 1.8}{0.2^2} = \\frac{-0.3}{0.04} = -7.5\n",
    "$$\n",
    "\n",
    "The model $s_{\\theta}(x_1)$ will attempt to predict this score, which helps it learn the distribution‚Äôs structure.\n",
    "\n",
    "For example, if the model's predicted score $s_{\\theta}(x_1) = -7.0$, the loss will be:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = ( -7.5 - (-7.0) )^2 = 0.25\n",
    "$$\n",
    "\n",
    "The model learns by minimizing this loss across many noisy data points.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Add More Noise and Repeat**\n",
    "Let's add more noise at time $t = 2$. Assume the noise variance increases to $\\sigma_2^2 = 0.4^2$.\n",
    "\n",
    "Now we add noise to $x_1$:\n",
    "\n",
    "$$\n",
    "x_2 = x_1 + \\epsilon_2, \\quad \\epsilon_2 \\sim \\mathcal{N}(0, 0.4^2)\n",
    "$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "x_2 = 1.8 + 0.5 = 2.3\n",
    "$$\n",
    "\n",
    "At each step, we continue this process of adding noise, and the model continues to learn how to denoise each noisy version $x_t$ back to the original $x_0$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Reverse the Process to Generate New Samples**\n",
    "After training, the model has learned how to denoise data. To generate a new sample:\n",
    "\n",
    "1. Start with pure Gaussian noise (e.g., $x_T \\sim \\mathcal{N}(0, 1)$).\n",
    "2. Use the learned score function to reverse the noise process:\n",
    "\n",
    "$$\n",
    "x_{T-1} = x_T - \\text{score}(x_T) \\Delta t\n",
    "$$\n",
    "\n",
    "3. Repeat the process step-by-step, gradually removing noise to generate a new sample.\n",
    "\n",
    "For example, if you start with $x_T = 3.0$ (pure Gaussian noise), the model will use the score function to reverse the diffusion, step-by-step, until it gets back to a sample like $x_0 = 1.5$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary:**\n",
    "- **Forward process:** Gradually add noise to the clean data.\n",
    "- **Modeling goal:** Train a model to predict the **score function** at each noise level.\n",
    "- **Reverse process:** Start with noise and use the learned score function to gradually denoise, generating new samples.\n",
    "\n",
    "This technique is powerful for generating high-quality data, like images, because it captures the underlying structure of the data distribution without needing to model the exact likelihood.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdeb5c1-78f0-4e9c-8a73-ff34032933eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
